---
title: "Fastai Movielens Lesson (from Scratch) with R and Reticulate"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

This notebook is an attempt to work through the MovieLens python notebook covered in the Fastai Deep Learning for Coders (Part 1) course by Jeremy Howard using R and Reticulate package. This content is covered in the following 2 video lectures in the course (lecture 5, lecture 6)

```{r}
# import R libraries
library(reticulate)
library(ggplot2)
library(dplyr)

```

```{r}

use_python("/home/paperspace/anaconda3/envs/fastai/bin/python", required = TRUE)
use_condaenv("fastai")
py_config()

main = import_main()
bi = import_builtins()
```

```{r}
# get relevant python imports
fstai_learner = import_from_path("fastai.learner", "../../fastai")
fstai_coldata = import_from_path("fastai.column_data", "../../fastai")
```

```{r}
py_run_string("
from fastai.learner import *
from fastai.column_data import *
              ")
```

```{r}
datapath = "../../data/ml-latest-small/"

ratings = read.csv(paste0(datapath, "ratings.csv"), stringsAsFactors = FALSE)
head(ratings)

movies = read.csv(paste0(datapath, "movies.csv"), stringsAsFactors = FALSE)
head(movies)
```

```{r}
Amat = matrix(c(1.0, 2.0, 3.0, 4.0), byrow = TRUE, ncol = 2)
Bmat = matrix(c(2.0, 2.0, 10.0, 10.0), byrow = TRUE, ncol = 2)

a = py$T(Amat)
a

b = py$T(Bmat)
b
```

```{r}
a$mul(b)
a$mul(b)$sum(1L) # note the use of 1L instead of 1 to ensure that we are passing integer
```

```{r}
py_run_string("
class DotProduct(nn.Module):
    def forward(self, u, m): return (u*m).sum(1)              
              ")
```

```{r}
model = main$DotProduct()
model$forward(a, b)
```

```{r}
u_unique = unique(ratings$userId)
user2idx = as.integer(seq(0, length(u_unique) - 1))
names(user2idx) = u_unique
user2idx[1:10]

m_unique = unique(ratings$movieId)
movie2idx = as.integer(seq(0, length(m_unique) - 1))
names(movie2idx) = m_unique
movie2idx[1:10]
```

```{r}
ratings$userIdx = user2idx[as.character(ratings$userId)]
ratings$movieIdx = movie2idx[as.character(ratings$movieId)]

n_users = length(u_unique)
n_movies = length(m_unique)
```

```{r}
py_run_string("
class EmbeddingDot(nn.Module):
    def __init__(self, n_users, n_movies, n_factors):
        super().__init__()
        self.u = nn.Embedding(n_users, n_factors)
        self.m = nn.Embedding(n_movies, n_factors)
        self.u.weight.data.uniform_(0,0.05)
        self.m.weight.data.uniform_(0,0.05)
        
    def forward(self, cats, conts):
        users,movies = cats[:,0],cats[:,1]
        u,m = self.u(users),self.m(movies)
        return (u*m).sum(1)              
              ")
```

```{r}
x = ratings[, c("userIdx", "movieIdx")]
y = np_array(ratings$rating)$astype(py$np$float32)
```

```{r}
val_idxs = py$get_cv_idxs(nrow(ratings))
val_idxs = as.integer(val_idxs)
n_factors = 50L
```

```{r}
data = py$ColumnarModelData$from_data_frame(datapath, val_idxs, r_to_py(x), y, c("userIdx", "movieIdx"), 64L)
```

```{r}
wd=1e-5
model = py$EmbeddingDot(n_users, n_movies, n_factors)$cuda()
opt = py$optim$SGD(model$parameters(), 1e-1, weight_decay=wd, momentum=0.9)
```

```{r}
py$fit(model, data, 3L, opt, py$F$mse_loss)
```

```{r}
py$set_lrs(opt, 0.01)
py$fit(model, data, 3L, opt, py$F$mse_loss)
```

```{r}
py_run_string("
def get_emb(ni,nf):
    e = nn.Embedding(ni, nf)
    e.weight.data.uniform_(-0.01,0.01)
    return e
              
class EmbeddingDotBias(nn.Module):
    def __init__(self, n_users, n_movies, n_factors, min_rating, max_rating):
        super().__init__()
        (self.u, self.m, self.ub, self.mb) = [get_emb(*o) for o in [
              (n_users, n_factors), (n_movies, n_factors), (n_users,1), (n_movies,1)
              ]]
        self.max_rating = max_rating
        self.min_rating = min_rating
              
    def forward(self, cats, conts):
        users,movies = cats[:,0],cats[:,1]
        um = (self.u(users)* self.m(movies)).sum(1)
        res = um + self.ub(users).squeeze() + self.mb(movies).squeeze()
        res = F.sigmoid(res) * (self.max_rating-self.min_rating) + self.min_rating
        return res              
              ")
```

```{r}
wd=2e-4
min_rating = min(ratings$rating)
max_rating = max(ratings$rating)
model = py$EmbeddingDotBias(n_users, n_movies, n_factors, min_rating, max_rating)$cuda()
opt = py$optim$SGD(model$parameters(), 1e-1, weight_decay=wd, momentum=0.9)
```

```{r}
py$fit(model, data, 3L, opt, py$F$mse_loss)
```

```{r}
py$set_lrs(opt, 0.01)
py$fit(model, data, 3L, opt, py$F$mse_loss)
```

```{r}
py_run_string("
class EmbeddingNet(nn.Module):
    def __init__(self, n_users, n_movies, n_factors, min_rating, max_rating, nh=10, p1=0.05, p2=0.5):
        super().__init__()
        (self.u, self.m) = [get_emb(*o) for o in [
            (n_users, n_factors), (n_movies, n_factors)]]
        self.lin1 = nn.Linear(n_factors*2, nh)
        self.lin2 = nn.Linear(nh, 1)
        self.drop1 = nn.Dropout(p1)
        self.drop2 = nn.Dropout(p2)
        self.min_rating = min_rating
        self.max_rating = max_rating
        
    def forward(self, cats, conts):
        users,movies = cats[:,0],cats[:,1]
        x = self.drop1(torch.cat([self.u(users),self.m(movies)], dim=1))
        x = self.drop2(F.relu(self.lin1(x)))
        return F.sigmoid(self.lin2(x)) * (self.max_rating-self.min_rating+1) + self.min_rating-0.5
           ")
```

```{r}
wd=1e-5
model = py$EmbeddingNet(n_users, n_movies, n_factors, min_rating, max_rating)$cuda()
opt = py$optim$Adam(model$parameters(), 1e-3, weight_decay=wd)
```

```{r}
py$fit(model, data, 3L, opt, py$F$mse_loss)
```

```{r}
py$set_lrs(opt, 0.01)
py$fit(model, data, 3L, opt, py$F$mse_loss)
```
